{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d70339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import swifter\n",
    "import json\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import wasserstein_distance\n",
    "import helpers as ph\n",
    "from termcolor import colored\n",
    "from helpers import PEW_SURVEY_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a249d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './data/human_resp/'\n",
    "RESULT_DIR = './data/runs'\n",
    "#CONTEXT = \"default\" # One of [\"default\", \"steer-qa\", \"steer-bio\", \"steer-portray\"]\n",
    "CONTEXT = \"steer-bio\" # One of [\"default\", \"steer-qa\", \"steer-bio\", \"steer-portray\"]\n",
    "OUTPUT_DIR = f'./data/distributions'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80175239",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT == \"default\":\n",
    "    SURVEY_LIST = [f'American_Trends_Panel_W{SURVEY_WAVE}' for SURVEY_WAVE in PEW_SURVEY_LIST] + \\\n",
    "                ['Pew_American_Trends_Panel_disagreement_500']\n",
    "else:\n",
    "    SURVEY_LIST = ['Pew_American_Trends_Panel_disagreement_500']\n",
    "    steer_df = pd.read_csv(f'./data/model_input/{CONTEXT}.csv',\n",
    "                       delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10ed27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPew_American_Trends_Panel_disagreement_500\u001b[0m\n",
      "\u001b[34m--Getting LM opinion distribution--\u001b[0m\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=100,context=steer-qa,num_train_trials=1,model=openai_ada,num_train_trials=22\n",
      "openai_ada steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=100,context=steer-qa,num_train_trials=1,model=openai_davinci,num_train_trials=22\n",
      "openai_davinci steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=10,context=steer-qa,num_train_trials=1,model=ai21_j1-grande,num_train_trials=22\n",
      "ai21_j1-grande steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=10,context=steer-qa,num_train_trials=1,model=ai21_j1-grande-v2-beta,num_train_trials=22\n",
      "ai21_j1-grande-v2-beta steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=100,context=steer-qa,num_train_trials=1,model=openai_text-davinci-002,num_train_trials=22\n",
      "openai_text-davinci-002 steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=100,context=steer-qa,num_train_trials=1,model=openai_text-ada-001,num_train_trials=22\n",
      "openai_text-ada-001 steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=100,context=steer-qa,num_train_trials=1,model=openai_text-davinci-003,num_train_trials=22\n",
      "openai_text-davinci-003 steer-qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carolina.oguchi/Documents/GitHub/opinions_qa/helpers.py:53: RuntimeWarning: divide by zero encountered in log\n",
      "  miss_value = np.log(min(min_prob, remaining_prob / Nmisses))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=100,context=steer-qa,num_train_trials=1,model=openai_text-davinci-001,num_train_trials=22\n",
      "openai_text-davinci-001 steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "opinions_qa:survey=Pew_American_Trends_Panel_disagreement_500,num_logprobs=10,context=steer-qa,num_train_trials=1,model=ai21_j1-jumbo,num_train_trials=22\n",
      "ai21_j1-jumbo steer-qa\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[34m--Getting human opinion distribution--\u001b[0m\n",
      "\u001b[34m--Comparing opinion distribution--\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply:   0%|          | 1/2266902 [00:01<1047:53:47,  1.66s/it]/var/folders/7w/89ypm2454vb_fhgcjxtqyq_r0000gp/T/ipykernel_8277/3556999448.py:58: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  combined_df['WD'] = combined_df.swifter.apply(lambda x: wasserstein_distance(x['ordinal'],\n",
      "Pandas Apply: 100%|██████████| 2266902/2266902 [02:27<00:00, 15388.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for SURVEY_NAME in SURVEY_LIST:\n",
    "    print(colored(SURVEY_NAME, \"red\"))\n",
    "    RESULT_FILES = [f for f in os.listdir(RESULT_DIR) if SURVEY_NAME in f and f'context={CONTEXT}' in f]\n",
    "    \n",
    "    ## Read human responses and survey info\n",
    "    info_df = pd.read_csv(os.path.join(DATASET_DIR, SURVEY_NAME, 'info.csv'))\n",
    "    info_df['option_ordinal'] = info_df.apply(lambda x: eval(x['option_ordinal']), axis=1)\n",
    "    info_df['references'] = info_df.apply(lambda x: eval(x['references']), axis=1)\n",
    "\n",
    "    md_df = pd.read_csv(os.path.join(DATASET_DIR, SURVEY_NAME, 'metadata.csv'))\n",
    "    md_df['options'] = md_df.apply(lambda x: eval(x['options']), axis=1)\n",
    "    md_order = {'Overall': {'Overall': 0}}\n",
    "    md_order.update({k: {o: oi for oi, o in enumerate(opts)} for k, opts in zip(md_df['key'], md_df['options'])})\n",
    "\n",
    "    ## Get model opinion distribution\n",
    "    print(colored('--Getting LM opinion distribution--', 'blue'))\n",
    "    model_df = ph.get_model_opinions(RESULT_DIR, RESULT_FILES, info_df)\n",
    "    \n",
    "    ## Get human opinion distribution\n",
    "    print(colored('--Getting human opinion distribution--', 'blue'))\n",
    "    if SURVEY_NAME != \"Pew_American_Trends_Panel_disagreement_500\":\n",
    "        resp_df = pd.read_csv(os.path.join(DATASET_DIR, SURVEY_NAME, 'responses.csv'))\n",
    "        human_df = pd.concat([ph.extract_human_opinions(resp_df, \n",
    "                                                        model_df, \n",
    "                                                        md_df, \n",
    "                                                        demographic=demographic, \n",
    "                                                        wave=int(SURVEY_NAME.split('_W')[1]))\n",
    "                   for demographic in ph.DEMOGRAPHIC_ATTRIBUTES])\n",
    "    else:\n",
    "        if CONTEXT != \"default\":\n",
    "            contexts = np.unique(model_df['context'])\n",
    "            steer_dict = ph.get_steering_group(CONTEXT, steer_df, contexts)\n",
    "            model_df['steer_attribute'] = model_df.apply(lambda x: steer_dict[x['context']]['attribute'], \n",
    "                                                 axis=1)\n",
    "            model_df['steer_group'] = model_df.apply(lambda x: steer_dict[x['context']]['group'], \n",
    "                                                         axis=1)\n",
    "            steer_groups = list(set(model_df['steer_group'].values)) + ['Overall']\n",
    "\n",
    "        human_df = []\n",
    "        for wave in PEW_SURVEY_LIST:\n",
    "            sn = f'American_Trends_Panel_W{wave}'\n",
    "            hdf = pd.read_csv(os.path.join(OUTPUT_DIR, f'{sn}_default_human.csv'))\n",
    "            idf = info_df[info_df['survey'] == f'Pew_{sn}']\n",
    "            hdf = hdf[hdf['qkey'].isin(idf['key'].values)]\n",
    "            human_df.append(hdf)\n",
    "        human_df = pd.concat(human_df)\n",
    "        if CONTEXT != \"default\": human_df = human_df[human_df['group'].isin(steer_groups)]\n",
    "        human_df['D_H'] = human_df.apply(lambda x: [float(f) for f in x['D_H'][1:-1].strip().split(' ') if len(f)], axis=1)\n",
    "        \n",
    "        \n",
    "    human_df.to_csv(os.path.join(OUTPUT_DIR, f'{SURVEY_NAME}_{CONTEXT}_human.csv'))\n",
    "    model_df.to_csv(os.path.join(OUTPUT_DIR, f'{SURVEY_NAME}_{CONTEXT}_model.csv'))\n",
    "\n",
    "    ## Combine and save\n",
    "    print(colored('--Comparing opinion distribution--', 'blue'))\n",
    "    combined_df = pd.merge(model_df, human_df)\n",
    "    combined_df['group_order'] = combined_df.apply(lambda x: md_order[x['attribute']][x['group']], axis=1)\n",
    "    combined_df['WD'] = combined_df.swifter.apply(lambda x: wasserstein_distance(x['ordinal'], \n",
    "                                                                         x['ordinal'],\n",
    "                                                                         x['D_M'], x['D_H']) / ph.get_max_wd(x['ordinal']), \n",
    "                                          axis=1)\n",
    "\n",
    "    combined_df.to_csv(os.path.join(OUTPUT_DIR, f'{SURVEY_NAME}_{CONTEXT}_combined.csv'))\n",
    "    \n",
    "    if CONTEXT == 'default':\n",
    "        print(colored('--Getting human baseline--', 'blue'))\n",
    "        human_overall = human_df[human_df['group'] == 'Overall'].rename(columns={'D_H': 'D_O', 'R_H': 'R_O'})\n",
    "        human_groups = human_df[human_df['group'] != 'Overall'].rename(columns={'D_H': 'D_G', 'R_H': 'R_G'})\n",
    "        key_to_ordering = {k: v for k, v in zip(info_df['key'], info_df['option_ordinal'])}\n",
    "        human_groups['ordinal'] = human_groups.apply(lambda x: key_to_ordering[x['qkey']], axis=1)\n",
    "        human_groups = pd.merge(human_groups, human_overall, on='qkey')\n",
    "        human_groups['WD'] = human_groups.apply(lambda x: wasserstein_distance(x['ordinal'], \n",
    "                                                                             x['ordinal'],\n",
    "                                                                             x['D_G'], x['D_O']) / ph.get_max_wd(x['ordinal']), \n",
    "                                              axis=1)\n",
    "        human_groups.to_csv(os.path.join(OUTPUT_DIR, f'{SURVEY_NAME}_baseline.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01418b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
